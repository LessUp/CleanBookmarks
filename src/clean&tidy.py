"""
ä¸€ä¸ªç”¨äºæ¸…ç†å’Œåˆ†ç±»æµè§ˆå™¨ä¹¦ç­¾çš„ Python è„šæœ¬ã€‚

è¯¥è„šæœ¬ä¼šè¯»å–ä¸€ä¸ª HTML æ ¼å¼çš„ä¹¦ç­¾æ–‡ä»¶ï¼Œæ ¹æ®é¢„å®šä¹‰çš„è§„åˆ™å¯¹ä¹¦ç­¾è¿›è¡Œåˆ†ç±»å’Œå»é‡ï¼Œ
ç„¶åç”Ÿæˆä¸€ä¸ªæ¸…ç†è¿‡çš„ã€å¯é‡æ–°å¯¼å…¥æµè§ˆå™¨çš„ HTML æ–‡ä»¶ï¼Œä»¥åŠä¸€ä¸ª Markdown æ ¼å¼çš„çº¯é“¾æ¥åˆ—è¡¨ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- åŸºäºå…³é”®è¯å¯¹ä¹¦ç­¾è¿›è¡Œå¤šçº§åˆ†ç±»ã€‚
- è‡ªåŠ¨ä¸ºç‰¹å®šåˆ†ç±»ï¼ˆå¦‚ GitHubï¼‰åˆ›å»ºæ›´æ·±å±‚æ¬¡çš„ç›®å½•ç»“æ„ã€‚
- æ¸…ç† URLï¼Œå»é™¤è·Ÿè¸ªå‚æ•°ã€‚
- å¯¹æ¸…ç†åçš„ URLè¿›è¡Œå»é‡ã€‚
- æ”¯æŒå‘½ä»¤è¡Œå‚æ•°ï¼Œå¯è‡ªå®šä¹‰è¾“å…¥/è¾“å‡ºæ–‡ä»¶ã€‚
"""
import os
import argparse
import json
import re
import glob
from bs4 import BeautifulSoup, Doctype
import time
from urllib.parse import urlparse
from collections import Counter
from functools import lru_cache

@lru_cache(maxsize=200_000)
def _parse_url_cached(url: str):
    """å¸¦ LRU ç¼“å­˜çš„ urlparseï¼ŒåŠ é€Ÿé‡å¤è§£æ"""
    return urlparse(url)


def clean_url(raw_url: str) -> str:
    """å»é™¤æŸ¥è¯¢å‚æ•°ä¸é”šç‚¹ï¼Œè¿”å›å‡€åŒ–åçš„ URL"""
    if not raw_url:
        return ""
    try:
        parts = _parse_url_cached(raw_url)
        # å¯¹äºæœ¬åœ°æ–‡ä»¶å’Œç‰¹æ®Šåè®®ï¼Œä¿ç•™åŸæ ·
        if parts.scheme in ('file', 'javascript', 'chrome', 'about'):
            return raw_url
        cleaned = f"{parts.scheme}://{parts.netloc}{parts.path}"
        return cleaned.rstrip("/")
    except Exception:
        return raw_url # è§£æå¤±è´¥åˆ™è¿”å›åŸå§‹ URL


def clean_title(title, config):
    """æ ¹æ®é…ç½®æ¸…ç†ä¹¦ç­¾æ ‡é¢˜"""
    cleaning_rules = config.get("title_cleaning_rules")
    if not cleaning_rules:
        return title
    
    # æ›¿æ¢è§„åˆ™ä¼˜å…ˆï¼Œå¤„ç† & ç­‰
    for old, new in cleaning_rules.get("replacements", {}).items():
        title = title.replace(old, new)

    for prefix in cleaning_rules.get("prefixes", []):
        if title.startswith(prefix):
            title = title[len(prefix):].lstrip()

    for suffix in cleaning_rules.get("suffixes", []):
        if title.endswith(suffix):
            title = title[:-len(suffix)].rstrip()

    return title.strip()


def classify_bookmark(url, title, seen_urls, config):
    """
    æ ¹æ®é…ç½®æ–‡ä»¶çš„è§„åˆ™å¯¹å•ä¸ªä¹¦ç­¾è¿›è¡Œåˆ†ç±»ã€‚è¿™æ˜¯ä¸€ä¸ªé‡‡ç”¨"åŠ æƒè¯„åˆ†æ¨¡å‹"çš„åˆ†ç±»å¼•æ“ã€‚
    å¢åŠ äº† `priority_rules` æ¥å¤„ç†å¿…é¡»ä¼˜å…ˆåŒ¹é…çš„è§„åˆ™ã€‚
    """
    if not url:
        return None, None

    cleaned_url = clean_url(url)
    if cleaned_url in seen_urls:
        return None, None
    seen_urls.add(cleaned_url)

    lower_title = title.lower()
    lower_url = url.lower()
    try:
        parsed = _parse_url_cached(url)
        domain = parsed.netloc.lower().replace("www.", "")
    except Exception:
        domain = ""

    scores = {}

    def apply_rules(rules_config, default_weight=1):
        for category_path_str, category_data in rules_config.items():
            category_path = tuple(category_path_str.split('/'))
            category_weight = category_data.get("weight", default_weight)

            for rule in category_data.get("rules", []):
                weight = rule.get("weight", category_weight)
                match_target_str = ""
                
                match_type = rule.get("match")
                if match_type == "domain":
                    match_target_str = domain
                elif match_type == "url":
                    match_target_str = lower_url
                elif match_type == "title":
                    match_target_str = lower_title
                elif match_type == "url_starts_with":
                    if any(lower_url.startswith(kw) for kw in rule.get("keywords", [])):
                        scores.setdefault(category_path, 0)
                        scores[category_path] += weight
                    continue
                elif match_type == "url_ends_with":
                    if any(lower_url.endswith(kw) for kw in rule.get("keywords", [])):
                        scores.setdefault(category_path, 0)
                        scores[category_path] += weight
                    continue
                elif match_type == "url_matches_regex":
                    if any(re.search(kw, lower_url) for kw in rule.get("keywords", [])):
                        scores.setdefault(category_path, 0)
                        scores[category_path] += weight
                    continue

                if not match_target_str:
                    continue
                
                # å…³é”®è¯åŒ¹é…
                keywords = rule.get("keywords", [])
                not_keywords = rule.get("must_not_contain", [])
                match_all = rule.get("match_all_keywords_in", {})

                if any(kw in match_target_str for kw in keywords):
                    if any(nkw in match_target_str for nkw in not_keywords):
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦åŒ¹é…æ‰€æœ‰é™„åŠ å…³é”®è¯
                    all_matched = True
                    for target, kws in match_all.items():
                        target_str = ""
                        if target == "title": target_str = lower_title
                        elif target == "url": target_str = lower_url
                        
                        if not all(kw in target_str for kw in kws):
                           all_matched = False
                           break
                    
                    if all_matched:
                        scores.setdefault(category_path, 0)
                        scores[category_path] += weight

    # 1. åº”ç”¨é«˜ä¼˜å…ˆçº§è§„åˆ™
    apply_rules(config.get("priority_rules", {}), default_weight=100)
    
    # å¦‚æœå·²æœ‰é«˜åˆ†åŒ¹é…ï¼Œåˆ™å¯èƒ½æå‰å†³å®š
    if scores:
        best_category_so_far = max(scores, key=scores.get)
        if scores[best_category_so_far] >= 100:
             return best_category_so_far, (url, title)

    # 2. åº”ç”¨æ™®é€šåˆ†ç±»è§„åˆ™
    apply_rules(config.get("category_rules", {}), default_weight=5)

    if not scores:
        return "æœªåˆ†ç±»", (url, title)

    best_category = max(scores, key=scores.get)
    return best_category, (url, title)


def build_structure(categorized_bookmarks, config):
    """
    æ ¹æ® categorized_bookmarks æ„é€ å¤šå±‚åµŒå¥— dictï¼Œå¹¶åº”ç”¨é¢†åŸŸé©±åŠ¨çš„äºŒæ¬¡åˆ†ç»„ã€‚
    ä¸¥æ ¼é™åˆ¶ç›®å½•å±‚çº§ä¸ºæœ€å¤šä¸¤å±‚ã€‚
    """
    structured = {}
    domain_grouping_rules = config.get("domain_grouping_rules", {})

    def add_nested(path_tuple, item):
        cur = structured
        # å¼ºåˆ¶æˆªæ–­ä¸ºæœ€å¤šä¸¤å±‚
        path_tuple = path_tuple[:2]
        for idx, key in enumerate(path_tuple):
            is_last = idx == len(path_tuple) - 1
            if key not in cur:
                # æœ€åä¸€å±‚æ‰åˆ›å»º _items
                cur[key] = {"_items": []} if is_last else {}
            if not isinstance(cur.get(key), dict):
                 cur[key] = {}
            cur = cur[key]
        if "_items" not in cur:
            cur["_items"] = []
        cur["_items"].append(item)

    for path, (url, title) in categorized_bookmarks:
        path_tuple = path if isinstance(path, tuple) else (path,)
        if not path_tuple:
            continue
            
        top_level_cat = path_tuple[0]
        final_path = path_tuple

        # åº”ç”¨é¢†åŸŸé©±åŠ¨åˆ†ç»„è§„åˆ™
        grouping_domains = domain_grouping_rules.get(top_level_cat)
        if grouping_domains:
            try:
                domain = _parse_url_cached(url).netloc.lower().replace("www.", "")
                if domain in grouping_domains:
                    final_path = (top_level_cat, domain)
            except Exception:
                pass
        
        add_nested(final_path, (url, title))
    
    return structured


def generate_markdown(structured_bookmarks, md_file):
    """é€’å½’ç”Ÿæˆå…·æœ‰æ ‡å‡†å¤šçº§æ ‡é¢˜å’Œæ’åºçš„ Markdown æ–‡ä»¶"""
    lines = ["# ä¹¦ç­¾æ•´ç†"]

    def walk(node, depth=1):
        # å¯¹å­ç›®å½•æŒ‰å­—æ¯é¡ºåºæ’åº
        sorted_keys = sorted([k for k in node.keys() if k != "_items"])

        # 1. å¤„ç†å­ç›®å½•
        for key in sorted_keys:
            heading_level = min(depth + 1, 4)
            heading_prefix = "#" * heading_level
            lines.append(f"\n{heading_prefix} {key}\n")
            walk(node[key], depth + 1)

        # 2. å¤„ç†å½“å‰å±‚çº§çš„ä¹¦ç­¾
        if "_items" in node:
            # å¯¹ä¹¦ç­¾æŒ‰æ ‡é¢˜æ’åº
            sorted_items = sorted(node["_items"], key=lambda item: item[1])
            for url, title in sorted_items:
                lines.append(f"- [{title}]({url})")

    # ä»é¡¶å±‚å¼€å§‹éå†
    top_order = [
        "å·¥ä½œå°", "AI", "æŠ€æœ¯æ ˆ", "ç”Ÿç‰©ä¿¡æ¯", "OnlineBooks", "æŠ€æœ¯èµ„æ–™", 
        "Lectures", "ç¤¾åŒº", "èµ„è®¯", "æ±‚èŒ", "å¨±ä¹", "OnlineTools", "å­¦ä¹ ", "ç¨åé˜…è¯»", "æœ¬åœ°ç½‘ç»œ & æµè§ˆå™¨", "æœªåˆ†ç±»"
    ]

    # æŒ‰æŒ‡å®šé¡ºåºå¤„ç†é¡¶å±‚åˆ†ç±»
    for cat in top_order:
        if cat in structured_bookmarks:
            lines.append(f"\n## {cat}\n")
            walk(structured_bookmarks[cat], depth=1)
    
    # å¤„ç†å…¶ä»–æœªåœ¨é¡ºåºåˆ—è¡¨ä¸­çš„åˆ†ç±»
    for cat in sorted(structured_bookmarks.keys()):
        if cat not in top_order:
             lines.append(f"\n## {cat}\n")
             walk(structured_bookmarks[cat], depth=1)

    with open(md_file, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))


def create_bookmark_html(structured_bookmarks, output_file):
    """æ ¹æ®æ„é€ å¥½çš„ä¹¦ç­¾ç»“æ„ï¼Œç”Ÿæˆ Netscape æ ¼å¼çš„ HTML æ–‡ä»¶ã€‚"""
    ts = str(int(time.time()))
    lines = [
        "<!DOCTYPE NETSCAPE-Bookmark-file-1>",
        "<META HTTP-EQUIV=\"Content-Type\" CONTENT=\"text/html; charset=UTF-8\">",
        "<TITLE>Bookmarks</TITLE>",
        "<H1>Bookmarks</H1>",
        "<DL><p>"
    ]

    def write_folder(name, content_dict, indent=1):
        ind = "    " * indent
        lines.append(f"{ind}<DT><H3 ADD_DATE=\"{ts}\" LAST_MODIFIED=\"{ts}\">{name}</H3>")
        lines.append(f"{ind}<DL><p>")
        
        # ç»Ÿä¸€æ’åºå­æ–‡ä»¶å¤¹å’Œä¹¦ç­¾
        sorted_keys = sorted([k for k in content_dict.keys() if k != "_items"])
        
        for key in sorted_keys:
            write_folder(key, content_dict[key], indent + 1)
        
        if "_items" in content_dict:
            sorted_items = sorted(content_dict["_items"], key=lambda item: item[1])
            for url, title in sorted_items:
                # HTMLå®ä½“ç¼–ç ï¼Œé˜²æ­¢ç‰¹æ®Šå­—ç¬¦ç ´åç»“æ„
                title_encoded = title.replace('&', '&').replace('<', '<').replace('>', '>')
                url_encoded = url.replace('&', '&')
                lines.append(f"{ind}    <DT><A HREF=\"{url_encoded}\" ADD_DATE=\"{ts}\">{title_encoded}</A>")

        lines.append(f"{ind}</DL><p>")

    top_order = [
        "å·¥ä½œå°", "AI", "æŠ€æœ¯æ ˆ", "ç”Ÿç‰©ä¿¡æ¯", "OnlineBooks", "æŠ€æœ¯èµ„æ–™", 
        "Lectures", "ç¤¾åŒº", "èµ„è®¯", "æ±‚èŒ", "å¨±ä¹", "OnlineTools", "å­¦ä¹ ", "ç¨åé˜…è¯»", "æœ¬åœ°ç½‘ç»œ & æµè§ˆå™¨", "æœªåˆ†ç±»"
    ]
    
    for cat in top_order:
        if cat in structured_bookmarks:
            write_folder(cat, structured_bookmarks[cat])
    
    for cat in sorted(structured_bookmarks.keys()):
        if cat not in top_order:
            write_folder(cat, structured_bookmarks[cat])

    lines.append("</DL><p>")

    with open(output_file, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))


def _count_bookmarks_recursive(node):
    """é€’å½’åœ°ç»Ÿè®¡ä¸€ä¸ªèŠ‚ç‚¹ä¸‹ä¹¦ç­¾çš„æ€»æ•°"""
    if not isinstance(node, dict): return 0
    count = len(node.get("_items", []))
    for key, value in node.items():
        if key != "_items":
            count += _count_bookmarks_recursive(value)
    return count

def print_statistics(structured_data, total_links_found, unique_links_count):
    """æ‰“å°å…³äºä¹¦ç­¾é›†åˆçš„ç»Ÿè®¡ä¿¡æ¯"""
    print("\nğŸ“Š----- ä¹¦ç­¾ç»Ÿè®¡ä¿¡æ¯ -----ğŸ“Š")
    
    duplicates_found = total_links_found - unique_links_count
    print(f"âœ¨ å»é‡æˆæœ: å…±æ‰¾åˆ°å¹¶ç§»é™¤äº† {duplicates_found} ä¸ªé‡å¤é“¾æ¥ã€‚")

    print("\nğŸ“š å„åˆ†ç±»ä¹¦ç­¾æ•°é‡:")
    category_counts = {}
    all_domains = []
    
    def collect_stats(node, cat_name):
        if not isinstance(node, dict): return
        category_counts[cat_name] = category_counts.get(cat_name, 0) + len(node.get("_items", []))
        
        for url, _ in node.get("_items", []):
            try:
                all_domains.append(_parse_url_cached(url).netloc.lower().replace("www.",""))
            except: pass
        
        for key, value in node.items():
            if key != "_items":
                collect_stats(value, f"{cat_name}/{key}")

    for category, content in structured_data.items():
        collect_stats(content, category)

    if category_counts:
        sorted_categories = sorted(category_counts.items(), key=lambda item: item[1], reverse=True)
        for category, count in sorted_categories:
            print(f"  - {category:<40} : {count} ä¸ª")

    if all_domains:
        print("\nğŸŒ æ‚¨æœ€å¸¸è®¿é—®çš„ Top 10 ç½‘ç«™:")
        top_10_domains = Counter(d for d in all_domains if d).most_common(10)
        for i, (domain, count) in enumerate(top_10_domains):
            print(f"  {i+1}. {domain} ({count} æ¬¡)")
        
    print("--------------------------\n")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="æ¸…ç†ã€åˆå¹¶å¹¶åˆ†ç±»æµè§ˆå™¨å¯¼å‡ºçš„ HTML ä¹¦ç­¾æ–‡ä»¶ã€‚",
        formatter_class=argparse.RawTextHelpFormatter
    )
    # ... (å‘½ä»¤è¡Œå‚æ•°è§£æéƒ¨åˆ†ä¸æ‚¨åŸæ–‡ä»¶ä¸€è‡´ï¼Œæ­¤å¤„çœç•¥ä»¥ä¿æŒç®€æ´)
    parser.add_argument('-i', '--input', nargs='+', default=[], help='è¾“å…¥çš„HTMLä¹¦ç­¾æ–‡ä»¶è·¯å¾„ã€‚')
    parser.add_argument('-c', '--config', default='config.json', help='åˆ†ç±»è§„åˆ™çš„JSONé…ç½®æ–‡ä»¶è·¯å¾„ã€‚')
    parser.add_argument('--output-html', default='tests/output/bookmarks_cleaned.html', help='è¾“å‡ºçš„HTMLæ–‡ä»¶è·¯å¾„ã€‚')
    parser.add_argument('--output-md', default='tests/output/bookmarks.md', help='è¾“å‡ºçš„Markdownæ–‡ä»¶è·¯å¾„ã€‚')
    args = parser.parse_args()

    input_files = args.input if args.input else glob.glob('tests/input/*.html')
    if not input_files:
        # å¦‚æœå‘½ä»¤è¡Œå’Œé»˜è®¤ç›®å½•éƒ½æ²¡æœ‰ï¼Œå°±ä½¿ç”¨æ‚¨ä¸Šä¼ çš„æ–‡ä»¶å
        if os.path.exists('bookmarks_2025_7_1.html'):
            input_files = ['bookmarks_2025_7_1.html']
            print(f"âœ… ä¿¡æ¯: æœªæ‰¾åˆ°è¾“å…¥æ–‡ä»¶ï¼Œä½¿ç”¨å½“å‰ç›®å½•ä¸‹çš„ 'bookmarks_2025_7_1.html'")
        else:
            print("âŒ é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•è¾“å…¥æ–‡ä»¶ã€‚")
            return
    
    config_file = args.config
    output_html_file = args.output_html
    output_md_file = args.output_md

    output_dir = os.path.dirname(output_html_file)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)

    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        print(f"âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_file}")
    except Exception as e:
        print(f"âŒ é”™è¯¯: åŠ è½½é…ç½®æ–‡ä»¶ '{config_file}' å¤±è´¥: {e}")
        return

    all_links = []
    print(f"\nğŸ“– å¼€å§‹å¤„ç† {len(input_files)} ä¸ªä¹¦ç­¾æ–‡ä»¶...")
    for input_file in input_files:
        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                content = f.read()
            soup = BeautifulSoup(content, 'lxml')
            links = soup.find_all('a')
            all_links.extend(links)
            print(f"   - å·²è¯»å–: {input_file} (æ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥)")
        except Exception as e:
            print(f"âš ï¸ è­¦å‘Š: å¤„ç†æ–‡ä»¶ '{input_file}' æ—¶å‡ºé”™: {e}")
    
    if not all_links:
        print("âŒ é”™è¯¯ï¼šæœªèƒ½åœ¨ä»»ä½•è¾“å…¥æ–‡ä»¶ä¸­æ‰¾åˆ°æœ‰æ•ˆçš„ä¹¦ç­¾é“¾æ¥ã€‚")
        return

    print(f"\nğŸ” å…±æ‰¾åˆ° {len(all_links)} ä¸ªä¹¦ç­¾é“¾æ¥ï¼ˆåˆå¹¶åï¼‰ã€‚")
    print("ğŸš€ å¼€å§‹åˆ†ç±»å’Œå»é‡...")

    categorized_bookmarks = []
    seen_urls = set()
    unclassified_log = []

    for link in all_links:
        url = link.get('href', '').strip()
        title = (link.string or url).strip()
        if url and title:
            category, item = classify_bookmark(url, title, seen_urls, config)
            if category and item:
                url_original, title_original = item
                cleaned_title = clean_title(title_original, config)
                categorized_bookmarks.append((category, (url_original, cleaned_title)))
                if category == "æœªåˆ†ç±»":
                    unclassified_log.append(f"{url_original} | {title_original}")
    
    if unclassified_log:
        log_file_path = "unclassified_log.txt"
        with open(log_file_path, "w", encoding="utf-8") as f:
            f.write("\n".join(unclassified_log))
        print(f"âš ï¸ æ³¨æ„: {len(unclassified_log)} ä¸ªä¹¦ç­¾è¢«å½’ä¸º'æœªåˆ†ç±»'ï¼Œè¯¦æƒ…è¯·è§ {log_file_path}")

    print(f"âœ… åˆ†ç±»å®Œæˆï¼å…±æ•´ç†å‡º {len(categorized_bookmarks)} ä¸ªæœ‰æ•ˆä¹¦ç­¾ã€‚")
    
    structured_data = build_structure(categorized_bookmarks, config)
    
    print_statistics(structured_data, len(all_links), len(seen_urls))
    
    print(f"ğŸ“ æ­£åœ¨ç”Ÿæˆ Markdown è¾“å‡º: {output_md_file}")
    generate_markdown(structured_data, output_md_file)
    
    print(f"ğŸŒ æ­£åœ¨ç”Ÿæˆ HTML è¾“å‡º: {output_html_file}")
    create_bookmark_html(structured_data, output_html_file)
    
    print("\nğŸ‰ å…¨éƒ¨å®Œæˆï¼æ‚¨çš„ä¹¦ç­¾å·²ç„•ç„¶ä¸€æ–°ã€‚")

if __name__ == '__main__':
    main()